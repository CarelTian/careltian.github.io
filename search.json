[{"categories":"论文阅读","content":"论文链接 In Search of an Understandable Consensus Algorithm (Extended Version) ","date":"2025-02-07","objectID":"/posts/doc/raft/:0:0","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"1、背景 共识算法是指能够让一组机器在一起协作并且能够容忍部分机器的故障。过去几十年，Paxos是最主流的共识算法，但是它较难理解，在现实中使用需要考虑到很多复杂的变化。于是提出了一种Raft的共识算法，其用于教学和实现更加简单。经过实验测试，在两个大学中对43个学生进行教学，有33个能够较好的理解并回答关于Raft的问题。 作者介绍了三个特性： 强领导机制：相比于其他共识算法，Raft具有更强领导人角色。意思是在一组机器中，选出一个领带人，所有的日志数据流由这个领导人发送给其他机器 领导选举：Raft使用随机计时器来选举领导，这个方法只是在心跳包机制中引入了一个小改动，能够更简单快速的解决冲突 成员变动：Raft在处理集群成员变动中使用了联合共识的方法，在调整过程中两组不同配置的大多数集群都会重叠，这样能够在配置变动中能够正常运行。 ","date":"2025-02-07","objectID":"/posts/doc/raft/:1:0","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"2、复制状态机 复制状态机是用来解决分布式系统中故障容错性，通常用来管理领导人选举和保存配置信息，使其在出现领导人宕机也能恢复存活。具体的例子有Chubby和Zookeeper。 如下图所示，client提交信息给Server，通常为leader，leader通过共识模块将数据转发给其他client，确保所有的机器的日志最终都保存了相同的请求和顺序，同时把执行的命令写入各自日志条目。当大多数节点确认后，将其标注为已提交。所有的节点将更改保存到各自的状态机，一旦命令被每个机器完整的复制，将输出结果发送给client。 复制状态极通常用复制日志来实现，每个服务器保存一组包含指令的日志，能够按照顺序执行。同时保证每个状态机执行的顺序和结果是一致的，内容也是相同。 共识算法的任务正是维护这种复制状态机，由以下特性： 安全性：在出现非拜占庭情况（恶意节点），如网络延迟，数据包丢失，重复等异常情况，不返回错误结果。 可用性：服务器之间能互相通信，在经典的五台服务区中可以允许任意2台故障。故障恢复后能重新加入集群。 容错性：不依赖时序，能处理时钟错误和消息延迟 及时性：一般情况，指令在收到大多数集群相应后快速完成，少速慢节点不影响系统性能。 ","date":"2025-02-07","objectID":"/posts/doc/raft/:2:0","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"3、Raft共识算法 中间有大篇幅批判Paxos，就此跳过。 Raft是一个管理复制日志的算法，通过选举领导人来实现数据一致性。当领导人收到来自客户端日志条目，会把日志复制给其他服务器，并告诉其他服务器什么时候可以把日志安全的保存在状态机中。总的来说，就是一个领导人管理整个系统，如果领导人挂了，再由新的服务器重新选举。 Raft可以将问题拆分为以下三个独立的子问题， 领导选举：当领导人发生故障的时候，一个新的领导人需要被选举出来，确保系统的连续性和稳定性 日志复制：领导人必须从客户端接收日志条目然后复制到集群中的其他节点，并强制要求其他节点的日志和自己保持一致。 安全性：通过状态机来保证安全，如果日志进入了状态机，那么其他服务器就不能在同样的日志索引使用相同命令 ","date":"2025-02-07","objectID":"/posts/doc/raft/:3:0","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"3.1 Raft基础概念 raft集群中的服务器在任何时候都包含三种状态，领导者，跟随者，候选者。正常的系统中，有一个领导者，其他跟随者是被动的，不能发起请求，只能被动回应领导者和候选者。领导者负责所有的请求，如果客户端向追随者发起请求则会转发给领导者。跟随者在收不到消息时，变为候选人启动选举，获得多数选票的候选人成为领导人。当领导人宕机，则降级为跟随者。 Raft将时间划分为任意长度的term，每个term开始于选举，选举出领导人管理集群直到任期结束。理论来说领导人只要一切正常，就能够一直继任。Term充当了逻辑始终的作用，序号随着时间递增。由于服务器之间保存的Term周期会不一致，服务器之间会通过交换term信息，小term会被大term覆盖。 Raft服务器之间使用RPC进行通信，仅需要两种类型。RequestVote RPC和AppendEntry RPC，分别用于投票和更新状态机 ","date":"2025-02-07","objectID":"/posts/doc/raft/:3:1","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"3.2 领导者选举 Raft使用心跳机制来触发领导者选举。服务启动初始状态为跟随者，跟随者永远是跟随者只要它能一直收到来自领导者和候选者的合法RPC。 追随者在一段时间（选举超时）没有收到消息后，开始发起选举。首先会增加它的周期然后变为竞选状态，然后投自己一票，再调用其他服务器的RequestVote，将一直保持这个状态直到出现， 赢得选举 另一个服务器赢得选举 一段时间没有决出胜负 一个服务器每个周期内只能投一票，先进先出原则，先看到谁的拉票就投谁。如果在投票中收到了领导者消息，观察它Term序号是否比自己的序号大，如果是则认可，如果不是则不予理会。 有种特殊情况，所有的跟随者同时都变成了候选者，选票就会分散，没有赢家。这时就会开启新一轮投票，Raft使用了随机延迟机制，每个服务器的选举会随机延迟（150-300ms），这样就能解决问题。 ","date":"2025-02-07","objectID":"/posts/doc/raft/:3:2","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"3.3 日志复制 选出领导者后开始处理客户端请求，每个请求携带一条被复制状态机执行的指令。领导者将指令作为新日志条目追加至日志中，并并行发起附加条目 RPC 给其他服务器复制，当日志条目被安全复制后，领导者将其状态机执行结果返回给客户端，如果跟随者宕机、延迟或网络丢包，领导者也会持续重试 RPC直至所有跟随者存储所有日志条目。（这里论文没说清楚，如果跟随者下线，一直重发太耗费资源了） 日志条目按序编号，包含创建时的任期号及待执行指令。日志条目在满足一定条件时变为可提交状态，即安全地应用到状态机中。领导人决定何时提交日志条目，Raft 算法保证所有提交条目持久化并最终被执行。日志条目在被复制到多数服务器时即被提交，包括前任领导人创建的条目。领导人追踪最大已提交条目索引，并在附加条目 RPC 中包含该索引，使跟随者同步应用已提交条目。通过两种方式保证日志的一致性，一是在一个日志索引对应一个条目，且内容和位置不会更改。二是使用AppendEntry RPC时，领导者会携带之前的条目索引和任期编号，如果跟随者找不到这条索引和任期编号就会拒绝日志条目。 下图展示了一个特殊的情况，a-b丢失了一些日志条目，c-d或者e有多余未提交的条目。f情况特殊，在term2中当选领导者，刚提交到日志就崩溃了，竞选到term3后提交了一些信息又崩溃了。Raft的处理办法是， 当AppendEntries RPC失败，领导者可以强制覆盖跟随者的日志，领导者为每个追随者维护一个尾指针，表示即将写入的位置。如果出现一致性检查失败，尾指针减一，直到和领导者一致，然后会删除冲突条目，追加领导者条目。 文中提出了一个没什么必要的优化方法，发生AppendEntries RPC冲突时，跟随者主动发送这个冲突任期内的第一个条目，直至不冲突，这样减小比对的次数。 ","date":"2025-02-07","objectID":"/posts/doc/raft/:3:3","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"论文阅读","content":"【未完待续】 ","date":"2025-02-07","objectID":"/posts/doc/raft/:4:0","tags":"Raft","title":"【论文阅读】Raft","uri":"/posts/doc/raft/"},{"categories":"随笔","content":" 注意 以下内容为作者在现有知识积累下的预测和推理，仅供个人思考，观点正确性请自行判断。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:0:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"1 背景 近三年来，人工智能领域迎来了蓬勃发展的新阶段。作为2022年12月首批体验ChatGPT的深度用户，我看到了这场技术革命如何重塑生产力。这场突破性进展正推动全球科研机构加速大模型研发竞赛，计算机视觉、语音交互等多模态应用也呈现加速发展态势。在技术融合创新的浪潮中，人类距离实现通用人工智能（AGI）的愿景似乎已不再遥远 与此同时，以Web3技术为代表的加密货币领域进入新一轮牛市，比特币已经突破了10万美元一枚，创下历史新高。这一现象级增长的核心驱动力来自于群体共识。无论它是否真的能给真实世界带来什么，只要有共识，有群体愿意认可其价值，那么它就是有价值的。但同样的，基于共识的价值体系也暗含脆弱性，如果政策风向、市场信心的出现转变，共识的崩塌就会大幅打击价格。 接下来，将对AI与Web3进行讨论。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:1:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"2 AI OpenAI o1和DeepSeek R1是基于强化学习实现了大模型思考推理能力，并且还能在思考中自我纠错。这个是实现AGI关键一步，因为人类的智力发展就是通过思考和推理逐渐形成。我认为实现AGI还有几个关键步骤，一是有自我驱动力能够自己提问寻找解答，二是永久训练，能够在推理过程中，自我更新模型参数，三是在学习中自发的形成某种结构，类似于人通过反复训练掌握了技能。现有的Ai训练和运行需要大量的内存，显存，高性能显卡，功耗越来越大，体积越来越大，也许未来生物计算机能有重大突破是个不错的方向。 大模型对生产力的提高是显而易见的，尤其对基础脑力劳动者而言。我不擅长前端开发，过去如果让我做个简单的界面，我会花大量时间查阅文档和教程，反复调试和验证。而现在借助大模型，只要掌握提问的技巧，常见的问题都能得到解决。以互联网行业为例，基础服务提供方在AI的帮助下，只需现有一半的员工就可以完成之前的工作量。因此，我认为未来基础岗位的缩减是必然的。客观来说，科技的进步是推动人类历史发展的重要力量。如果AI能够像预测蛋白质结构那样，改造和创新现有的技术，那么第四次工业革命就已经开始了 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:2:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"3 Web3 我对区块链技术有过研究，其本质就是分布式账本技术。中本聪建立它的背景在2008年金融危机，目的在于创造一个去中心金融体系，解决传统金融体系的信任问题，但如今看来并没有解决这个问题。目前对web3主流开发社区停留在以太坊的项目，例如NFT数字资产，铭文等。这里面鱼龙混杂，粗糙的图片，一段奇怪的音频可以拍卖到几十万美元。我认为真正的价值是可以映射到现实世界的，曾看到国内有把NFT门票用作区块链线下活动的入场券，但是这存在一些问题，由于区块链是完全匿名的，如果注册多个地址一个人就可以购买成百上千张票，从而黄牛就能高价转卖给别人。并且由于完全匿名性，黑灰活动追踪溯源困难。所以目前来看去中心化应用还处于早期探索阶段，并没有被大众广为使用。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:3:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"4 演化 如果让我选择方向，我会all in 去中心化应用，web3，分布式系统。AI的前景虽然很好，但是这是属于头部企业的蛋糕。算力垄断，技术垄断下创业公司能产出重大突破几乎不可能。而选择分布式应用的理由如下，目前用户的隐私数据包括行为喜好被大型企业收集，在信息即价值的时代会形成强大的垄断市场，出现不正当利益和竞争。我理解的去中心化应用是由社区集群来作为网络服务的提供者和维护者，它们由十个左右独立的组织或个体通过定制化的共识协议连接在一起来为用户提供服务，类似于区块链中的联盟链。这样做的的好处是，整个系统的源代码和算法向用户公开，用户可以选择所需要的服务，而不局限于垄断资本设定的规则协议。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:4:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"4.1 数字ID与统一信用体系 区块链技术虽然具备去中心化与隐私保护的优势，但也因受不法分子利用而诟病。比特币的市值约为2万亿美元，而全球每年的洗钱金额也被估计为2万亿美元。把区块链技术改造成具备实用性的方案，一个折中的方法能兼顾隐私保护与身份追踪。 具体来说，可以使用个人ID（ID card, Driver License）和人脸识别生成一个临时数字ID。基于这一信息创建的地址包含了其他用户无法追踪但政府部门能够追踪的密码。这一密码可通过与数据库中预留的密钥解密得到用户的真实身份信息。该地址既可以理解为比特币中的地址，也可以被视为互联网中的用户名。借助现有密码学技术，这种方案是可行的。 此外，信用体系的建立在现实中已有不少成功案例，例如蚂蚁信用积分、闲鱼信用等级等。然而，这些系统对用户而言往往缺乏透明性，评分与等级计算方式不为人知。相比之下，Uber 的双向评分机制则更具借鉴意义。在Uber中，司机与乘客可以相互评分，并将评分结果直接展示给每轮服务的使用者。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:5:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"4.2 C2C 点对点交易（C2C）指的是用户对用户的业务模型，不依赖于第三方机构。当今的点对点服务已有一定的雏形，但本质上仍依赖于平台作为担保方，有的甚至是聊天记录作为口头协议。我认识的不少人曾通过这种方式完成交易。例如，网络上两个素不相识的人，一方发布帖子寻求合租，另一方看到后与之沟通，双方一拍即合，认为彼此已经达成共识，但并没有任何风险保障程序。未来，C2C模式有望发展得更加成熟。两个用户可以通过第三方平台交流，当达成一致意见时签署电子协议。该协议由双方密钥各自加密两次，生成两份副本发送至去中心化网络，并同时提交一定数量的Token作为保证金。在协议完成后，双方可以进行一次互评，以完善信用体系。如果出现纠纷，平台将提供记录和证据链支持，帮助双方解决争议。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:6:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"随笔","content":"4.3 大数据和需求匹配 设想下面一个场景，由于刚刚搬家到新城市你把快递默认地址填错了，直到最后才发现。经过计算，亲自去取成本太高。申请退货重新发货，时间拖的太晚。一个理想的解决方案是，找在那个城市的朋友帮忙转寄过来。但问题是，如果没有呢？ 一种有效的解决方案是需求匹配，这在当前已有成功的案例可循。例如，在外卖服务中，用户下单后，信息同步至商家和平台。平台根据地理位置、订单地址、报酬以及完成时限等要素，将订单推送给外卖员供其选择。这个模式可以进一步扩展到更多领域，例如快递代寄、临时杂务等。在理想的场景中，用户可以直接发布需求，设定赏金，并将信息发送至一个去中心化的网络。通过相关性匹配算法，这些需求能够精准推送给适合的人群。此外，为了确保系统的高效与安全，可以借助ai自动化判断需求的合法性、出价的合理性，并实现更细粒度的智能匹配。 ","date":"2025-02-05","objectID":"/posts/doc/prediction/:7:0","tags":"预测","title":"互联网未来10年的展望","uri":"/posts/doc/prediction/"},{"categories":"论文阅读","content":"1 论文链接 https://arxiv.org/abs/1706.03762 ","date":"2024-04-18","objectID":"/posts/doc/transformer/:1:0","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"2 简介 早期普遍使用RNN来处理翻译任务，它是把输入和输出序列化为token，然后对每个token逐步计算。输入为上一轮计算的状态和当前的输入token，输出为下一轮的状态和输出token。所谓token就是一个单词或一个分词，起始符，终止符等组成，然后映射到一个具体的数值表中。这个结构的输入和输出也被称为编码器-解码器架构。 RNN的主要问题在于编码器和解码器中间通过一个状态传递信息，在处理长序列问题中计算效率和表现不太行，而且必须串行执行，效率低。于是，作者提出了基于注意力机制Transformer架构。它能够并行化训练，在8块P100训练12小时就能取得不错的效果。 ","date":"2024-04-18","objectID":"/posts/doc/transformer/:2:0","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"3 模型结构 编码器由6个相同的层组成，图中仅展示了一个子层，每个子层中第一个是多头注意力机制捕捉不同位置之间的全局依赖关，第二个是全连接前馈神经网络主要用于非线性变换和特征提取。两个子层都使用了 层归一化和残差连接来保持梯度稳定并提升训练效率，可能是为了在深层网络中保持有效梯度传递和加速收敛。全连接层的作用是为了低维数据(512)扩展到高维（2048）输出。 ","date":"2024-04-18","objectID":"/posts/doc/transformer/:3:0","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"3.1 嵌入层 图中的embedding layer来源过程如下，句子-\u003etoken化-\u003etoken ID-\u003e嵌入层（d=512)，这样一个词就由一个512维的稠密向量，例如[1.212, 3.234, 5.111, …]所对应的表示“我”。这样一段句子本质上就是一个嵌入矩阵作为输入。Positional Encoding是为了捕捉单词之间的顺序关系，计算公式如下，pos是位置，i是嵌入向量的索引，d=512。 \\begin{align} PE_{(pos, 2i)} \u0026= \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right) \\ PE_{(pos, 2i+1)} \u0026= \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right) \\end{align} ","date":"2024-04-18","objectID":"/posts/doc/transformer/:3:1","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"3.2 多头注意力 文中的多头注意力是由8个缩放点乘积注意力合并得到的，先说缩放点积注意力。Q,K,V是一组抽象的概念，Q直观说就是生成的方向，K用于输入信息匹配，QK内积得到当前位置哪个输出信息最相关，一轮计算得到两个内容，注意力输出表示全局上下文重要信息，权重表示查询位置Q对K的重要程度。 \\begin{align} \\text{Attention}(Q, K, V) \u0026= \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V \\end{align} ","date":"2024-04-18","objectID":"/posts/doc/transformer/:3:2","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"3.3 前馈神经网络 扩大维度至2048，捕捉更复杂的特征。Relu将函数变为非线形运算是精髓，是产生智能学习质变的关键之一。 \\begin{align} FFN(x) \u0026= \\max(0, x W_1 + b_1) W_2 + b_2 \\end{align} ","date":"2024-04-18","objectID":"/posts/doc/transformer/:3:3","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"3.4 注意力在模型中的应用 文中用自注意力层和循环层，卷积层做比较，都是把一个向量映射到另一个向量，比如隐藏层在编码器和解码器之间的作用。通过对计算复杂度，并行化，最长路径网络的比较，自注意力的时间复杂度明显是比其余两者更低的。 Tranformer运用多头注意力在下面三个方面 在编码器-解码器层中，Q来自于上一个解码层的输出，而K,V来源于上一个解码层输出。这样使得解码器的每个位置都能获取输入序列的所有位置。 编码器中的自注意力层，所有的Q，K，V来自于相同的来源。编码器中，它们来源于前一层的输出，第一层来源于嵌入层。这样做生成某个位置的单词时可以考虑输入序列的任意位置单词，捕捉长距离依赖关系。 解码器的自注意力层同样能能关注到第一个位置到当前输出位置的信息，为了保证自回归，训练中不能提前知道未来的词，将未来位置的注意力权重设置负无穷，softmax就会输出0的概率。 ","date":"2024-04-18","objectID":"/posts/doc/transformer/:3:4","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"4 训练 使用了Adam优化器，β1 = 0.9, β2 = 0.98 and ϵ= 10−9和动态学习率算法。 正则化方法，残差连接+每个子层dropout概率0.1+标签平滑处理 ","date":"2024-04-18","objectID":"/posts/doc/transformer/:4:0","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"5 评价 相比于RNN和CNN最显著的优点就是能够并行化，缩短计算时间，并且能够合适地处理长序列，捕捉上下序列重要信息。我认为他的缺点是必须要提供大量的数据集才能有较理想的效果，个人设备训练时很容易因数据集不足或模型参数少造成过拟合。 ","date":"2024-04-18","objectID":"/posts/doc/transformer/:5:0","tags":"transformer","title":"【论文阅读】Attention Is All You Need","uri":"/posts/doc/transformer/"},{"categories":"论文阅读","content":"论文链接 MapReduce: Simplified Data Processing on Large Clusters ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:0:0","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"1 背景 2004年正处于互联网起步阶段，谷歌公司为了处理大量的元数据（文档、日志、摘要）需要成百上千台机器处理。这时需要设计一个程序，能够让分布在不同位置的机器并行处理分布式的数据，同时要有容错，简化计算的代码。受到Lisp语言中函数式编程的启发，创造了map和reduce两种操作来处理输入数据。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:1:0","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"2 编程模型 整个模型的输入和输出都是Key/Value形式 Map: 一个函数由用户编写，输入为分割后的数据块，计算得到中间结果对key/value，然后分组合并发送给Reduce函数 Reduce：同样为用户编写，接受中间键值对，根据key来合并组成一个更小的集合，然后输出最终结果。 map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, “1″); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); 下图以一个简单的例子展示词频统计的数据流动过程。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:2:0","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"3 实现 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:3:0","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"3.1 执行概述 下图以不同职能划分的视角解释MapReduce的过程， MapReduce库首先将输入文件拆分为 M 个部分，每个部分通常为 16 MB 到 64 MB（具体大小可通过可选参数由用户控制）。随后，它在 一组集群机器上启动多个程序副本进行处理 其中一个程序主节点，其余的是工作节点，由主节点分配任务。任务包括 M 个 map 任务 和 R 个 reduce 任务。主节点会选择空闲的工作节点，并为其分配 map 或 reduce 任务。 被分配到map任务的工作节点会读取对应输入分片的内容，对输入数据进行解析，将其拆分为 键值对并将每个键/值对传递给用户定义的 Map 函数。Map 函数生成的中间键值对会暂时缓存在内存中。 缓冲的键值对会被定期写入本地磁盘，并通过分区函数划分为 R 个区域。这些缓冲键值对在本地磁盘上的位置会返回给主节点，主节点负责将这些位置信息转发给 reduce 工作节点。 当主节点通知 reduce 工作节点这些位置后，reduce 节点通过远程过程调用从 map 工作节点的本地磁盘读取缓冲数据。当 reduce 节点读取完所有中间数据后，会根据中间键对数据进行排序，以便将相同键的所有数据分组到一起。由于通常许多不同的键会映射到同一个 reduce 任务，因此排序是必要的。如果中间数据量过大，无法全部加载到内存中，则使用外部排序。 reduce 工作节点遍历排序后的中间数据，对于每个遇到的唯一中间键，将该键及其对应的中间值集合传递给用户定义的 Reduce 函数。Reduce 函数的输出会被追加到该 reduce 分区的最终输出文件中。 当所有的 map 任务和 reduce 任务都完成后，主节点唤醒用户程序。此时，用户程序中的 MapReduce 调用返回到用户代码。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:3:1","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"3.2 master数据结构 它存储每一个map和reduce 任务的状态（空闲、工作中或完成)，以及 worker 机器 (非空闲任务的机器) 的标识。 主节点是中间文件区域位置从 map 任务传播到 reduce 任务的桥梁。因此，对于每个完成的 map 任务，主节点会存储该任务生成的 R 个中间文件区域的位置和大小信息。随着 map 任务的完成，主节点持续接收这些位置和大小信息的更新，并将信息逐步推送给正在执行 reduce 任务的工作节点。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:3:2","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"3.3 容错机制 worker故障：master周期性ping每个worker，如果出现没有相应，则被认定为故障 出现故障后，master将故障worker正在执行或已产出的map标记为失效，重新调度分配给其他worker。如果产出的reduce已完成，则被视为有效产出 master故障：周期性将数据结构写入磁盘，以最小化损失。出现故障终止程序，等到新master运行接替此检查点继续运行。 出现故障的语义：由于MapReduce操作是原子性的，可以保证输出的正确性和顺序一致。大多数情况可以忽略这方面问题。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:3:3","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"3.4 存储位置 GFS——多台机器组成的分布式文件系统。将每个文件分钟64MB的块，将多个副本(3份)保存在不同机器中。master在调度的时候会考虑避免网络传输开销，优先在含有map所需文件的机器 执行。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:3:4","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"3.5 支援任务 指的是某些机器由于网络差，磁盘读写故障等原因拖后腿，速度远远其他worker。当执行到最后阶段时，无论这个拖油瓶是否正在运行，直接将任务重新分配给其他worker，取最先完成的输出文件为准。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:3:5","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"4 改进 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:4:0","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"4.1 分区 使用hash(key) mod R分区，能够比较公平的把相同的Key分到同一个区，并且解决负载均衡问题。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:4:1","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"4.2 排序 每个分区内保证是根据Key来排序的，这有利于在最终输出文件时支持随机访问。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:4:2","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"论文阅读","content":"4.3 Combiner 函数 一种提高I/O效率的方法，在本地将中间结果合并后再转发给Reduce，减小网络传输压力和磁盘读写压力。 ","date":"2024-02-18","objectID":"/posts/doc/mapreduce/:4:3","tags":"MapReduce","title":"【论文阅读】MapReduce","uri":"/posts/doc/mapreduce/"},{"categories":"笔记","content":"Robot Framework简介 RPA框架是机器人过程自动化(RPA)的开源库和工具的集合，它被设计为与Robot Framework和Python一起使用。目标是为软件机器人开发人员提供良好的文档和积极维护的核心库。相比较于纯python实现，它的使用逻辑清晰，使用方法简单，可扩展性高。但是在某些特殊场景，Robot Framework具有局限性，比如并发执行，直接接管浏览器而不需要重新打开。 以下Robot Framework简称RF。RF的应用分为两种，一个是为公司解放劳动力，自动化处理简单但又繁琐的操作。二是为个人使用，现电商抢购活动还有秒杀抢单我认为完全可以用RF实现的。 ","date":"2023-04-10","objectID":"/posts/doc/rf/:1:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"RF准备工作 ","date":"2023-04-10","objectID":"/posts/doc/rf/:2:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"环境 项目 版本 python 3.7.9 rpaframework 22.2.3 robotframework 5.0.1 selenium 4.5.0 ","date":"2023-04-10","objectID":"/posts/doc/rf/:3:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"开发工具 RF实体是一个.robot文件，pycharm的插件有点问题，而且不能识别里面的Keyword。因此使用Vscode进行开发。 需要准备的插件如下所示 ","date":"2023-04-10","objectID":"/posts/doc/rf/:4:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"RF结构 RF分为四部分Settings, Variables, Test Cases, Keywords ","date":"2023-04-10","objectID":"/posts/doc/rf/:5:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"*** Setting *** Documentation 用于机器人的描述和介绍，对执行并没有什么作用 Library 用于导入各种库，每个库会有上百个Keyword Library的官方文档（https://rpaframework.org/index.html） ","date":"2023-04-10","objectID":"/posts/doc/rf/:6:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"*** Variable *** 用于程序初始化的变量 @ 用作创建列表 \u0026 用作创建字典 $ 用作创建字符串或数字 ","date":"2023-04-10","objectID":"/posts/doc/rf/:7:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"*** Test Cases *** 与Task作用一样，用于写总流程 ","date":"2023-04-10","objectID":"/posts/doc/rf/:8:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"***Keywords *** 由多个自定义的Keyword组成，表示执行的步骤。 定义的Keyword下行加[Arguments] ${} 可作为函数的参数。 循环表示方法 FOR ${} IN @{} do something ${} END 判断表示方法 IF condititon do() ELSE IF condition do() ELSE do() END 异常处理 TRY Some Keyword EXCEPT ValueError: * type=GLOB AS ${error} Error Handler 1 ${error} EXCEPT [Ee]rror \\\\d+ (Invalid|Bad) usage type=REGEXP AS ${error} Error Handler 2 ${error} EXCEPT AS ${error} Error Handler 3 ${error} END ","date":"2023-04-10","objectID":"/posts/doc/rf/:9:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"Python扩展 导入库时使用的测试库的名称与实现它的模块或类的名称相同。 ","date":"2023-04-10","objectID":"/posts/doc/rf/:10:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"Library from robot.api.deco import library RF导入python的库就像直接实例化一个类，运行构造参数，如下图所示。 @library装饰器 配置实现为类的库的一种简单方法是使用robot.api.deco.library类装饰器。它允许配置库的作用域、版本、自定义参数转换器、文档格式和监听器，可选参数scope、version、converter、doc_format和监听器。当使用这些参数时，它们会自动设置匹配的ROBOT_LIBRARY_SCOPE、ROBOT_LIBRARY_VERSION、ROBOT_LIBRARY_CONVERTERS、ROBOT_LIBRARY_DOC_FORMAT和ROBOT_LIBRARY_LISTENER属性 ","date":"2023-04-10","objectID":"/posts/doc/rf/:11:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"Keyword from robot.api.deco import keyword 默认情况下，一个python类或模块下的所有函数被认为是Keyword。如果在设置中使用下图设置，默认不配置为keyword。函数的前缀可以使用@keyword开启。 或者直接使用@not_keyword禁用RF。 @keyword(name=None,tag=(),type=())-\u003eAny 可以修改把参数放在name里面 ","date":"2023-04-10","objectID":"/posts/doc/rf/:12:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"常用的库 ","date":"2023-04-10","objectID":"/posts/doc/rf/:13:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"RPA.Browswer.Selenium auto_close=${FALSE} //执行完不自动关闭 打开网站 Open Available Browser url 输入内容 Input Text locator text clear=True 下拉框选择 Select From List By Value locator values 单选按钮 Select Radio Button group_name value 点击元素 Click Element locator id:example name:example xpath://div[@id=“example”] css:div#example 直接提交页面的唯一表单 Submit Form 截图 Screenshot locator output 等待元素出现 Wait Until Page Contains Element locator timeout=None ","date":"2023-04-10","objectID":"/posts/doc/rf/:14:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"RPA.Excel.Files 打开excel Open Workbook path 读取返回表格 Read Worksheet As Table name=None header=False start=None 创建excel Create Workbook path fmt=xlsx sheet_name=None 设置表格值 Set Cell Value row column value 获取表格值并返回 Get Cell Value row column name=active sheet ","date":"2023-04-10","objectID":"/posts/doc/rf/:15:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"参考资料 Robot Framework User Guide（http://robotframework.org/robotframework/latest/RobotFrameworkUserGuide.html） RPA Documentation, Training Courses, Certificates | Robocorp(https://robocorp.com/docs) Keyword libraries(https://robocorp.com/docs/libraries) XPATH定位的用法(https://www.cnblogs.com/aiyiless/p/16111340.html) ","date":"2023-04-10","objectID":"/posts/doc/rf/:16:0","tags":"robot","title":"RF学习总结","uri":"/posts/doc/rf/"},{"categories":"笔记","content":"iptables简介 netfilter/iptables（简称为iptables）组成Linux平台下的包过滤防火墙，与大多数的Linux软件一样，这个包过滤防火墙是免费的，它可以代替昂贵的商业防火墙解决方案，完成封包过滤、封包重定向和网络地址转换（NAT）等功能。 ","date":"2021-03-01","objectID":"/posts/doc/iptables/:1:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"iptables基础 ​ 规则（rules）其实就是网络管理员预定义的条件，规则一般的定义为“如果数据包头符合这样的条件，就这样处理这个数据包”。规则存储在内核空间的信息 包过滤表中，这些规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等。当数据包与规 则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。配置防火墙的 主要工作就是添加、修改和删除这些规则。 ","date":"2021-03-01","objectID":"/posts/doc/iptables/:2:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"iptables命令的管理控制选项 -A 在指定链的末尾添加（append）一条新的规则 -D 删除（delete）指定链中的某一条规则，可以按规则序号和内容删除 -I 在指定链中插入（insert）一条新的规则，默认在第一行添加 -R 修改、替换（replace）指定链中的某一条规则，可以按规则序号和内容替换 -L 列出（list）指定链中所有的规则进行查看 -E 重命名用户定义的链，不改变链本身 -F 清空（flush） -N 新建（new-chain）一条用户自己定义的规则链 -X 删除指定表中用户自定义的规则链（delete-chain） -P 设置指定链的默认策略（policy） -Z 将所有表的所有链的字节和数据包计数器清零 -n 使用数字形式（numeric）显示输出结果 -v 查看规则表详细信息（verbose）的信息 -V 查看版本(version) -h 获取帮助（help） ","date":"2021-03-01","objectID":"/posts/doc/iptables/:3:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"防火墙处理数据包的四种方式 ACCEPT 允许数据包通过 DROP 直接丢弃数据包，不给任何回应信息 REJECT 拒绝数据包通过，必要时会给数据发送端一个响应的信息。 LOG在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则 ","date":"2021-03-01","objectID":"/posts/doc/iptables/:4:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"iptables 官方文档 https://netfilter.org/ ","date":"2021-03-01","objectID":"/posts/doc/iptables/:5:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"屏蔽外网IP 下面介绍一种方法只有国内的IP才能连接服务器，对防火墙建设有参考价值 ","date":"2021-03-01","objectID":"/posts/doc/iptables/:6:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"背景 在我刚接触云产品时，使用的是腾讯云赠送的一个月云服务器。这一个月时间里我似乎也没部署过什么项目，就照着书上和网上的资料稍微捣鼓了下Ubuntu，但是没有使用网络服务的。可就在短短一个月内，有次上机检查，就发现cpu占用率99%，接着腾讯云给我发了条消息，说是中了挖矿病毒。查看日志后，来自卢森堡的一个IP成功登录。 后来我在阿里云租了个云服务器，在上面部署了C/S网络通信类的程序。刚开始网络编程功底不熟，没有对各种异常处理，如果没接收到预期格式的数据包就会报错。我使用的是一个很隐蔽的端口，但是经常会收到全球各地的连接，只要对方一发送数据包，服务器执行的程序就会报错退出。 再后来，我写了个网站，日志中经常能看到会有很多来自国外的爬虫进来逛。那么有没有什么方法屏蔽所有国外的连接呢？ ","date":"2021-03-01","objectID":"/posts/doc/iptables/:7:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"建立一条规则链 创建一条规则链mylink，加到入站的规则中 iptables -N mylink iptables -A INPUT -j mylink ","date":"2021-03-01","objectID":"/posts/doc/iptables/:8:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"获取国内所有的IP 获取所有国内的ip网段，保存到china_ssr.txt文件中 wget -q --timeout=60 -O- 'http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest' | awk -F\\| '/CN\\|ipv4/ { printf(\"%s/%d\\n\", $4, 32-log($5)/log(2)) }' \u003e /root/china_ssr.txt ","date":"2021-03-01","objectID":"/posts/doc/iptables/:9:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":"笔记","content":"shell脚本 观察ip网段，使用的是网络前缀格式，正好满足iptables的命令格式。用vim t跳转到末尾，发现竟然有8600多行国内网段。那么首先排除一个个手动输入命令的可能，于是我写了个脚本。 while read line do iptables -A mylink -s $line -j ACCEPT done \u003c china_ssr.txt 上面的脚本逐行读取文件的内容，然后执行命令。经过测试，发现iptables的优先级是自顶向下的，即当前规则必须是上面规则的子集，不然就无效。现在已经把所有国内IP允许访问，接下来就禁止全网IP。 iptables -A mylink -j DROP 上面这条命令极其危险。一定要放在链的末尾。本人就是输入了这条命令，导致无法连接到云服务器，无奈之下到阿里云回滚两个月前的快照。 ","date":"2021-03-01","objectID":"/posts/doc/iptables/:10:0","tags":"网络安全","title":"iptables屏蔽所有国外IP!网络安全的利器！","uri":"/posts/doc/iptables/"},{"categories":null,"content":"分布式系统技术爱好者，理想主义者 创建这个博客的目的是为了留下思考、学习、想法的记录。人对于过去的感受只存在于记忆，但是记忆会遗忘。所以利用博客归纳和整理，多记录些值得保留的部分。 ","date":"0001-01-01","objectID":"/me/:0:0","tags":null,"title":"","uri":"/me/"}]