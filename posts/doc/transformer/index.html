<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-cn">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <style>
      :root {
        --background-light: url('/images/day.jpg');
        --background-dark: url('/images/dark.jpg');
      }
    </style>
    <title>【论文阅读】Attention Is All You Need - Sky0x7d1</title><meta name="author" content="ZergFlood">
<meta name="description" content="1 论文链接 https://arxiv.org/abs/1706.03762
"><meta name="keywords" content='116, 114, 97, 110, 115, 102, 111, 114, 109, 101, 114'>
  <meta itemprop="name" content="【论文阅读】Attention Is All You Need">
  <meta itemprop="description" content="1 论文链接 https://arxiv.org/abs/1706.03762">
  <meta itemprop="datePublished" content="2024-04-18T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-04-18T00:00:00+00:00">
  <meta itemprop="wordCount" content="89">
  <meta itemprop="keywords" content="Transformer"><meta property="og:url" content="https://careltian.github.io/posts/doc/transformer/">
  <meta property="og:site_name" content="Sky0x7d1">
  <meta property="og:title" content="【论文阅读】Attention Is All You Need">
  <meta property="og:description" content="1 论文链接 https://arxiv.org/abs/1706.03762">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-18T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-04-18T00:00:00+00:00">
    <meta property="article:tag" content="Transformer">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="【论文阅读】Attention Is All You Need">
  <meta name="twitter:description" content="1 论文链接 https://arxiv.org/abs/1706.03762">
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="img/profile1.jpg"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" type="text/html" href="https://careltian.github.io/posts/doc/transformer/" title="【论文阅读】Attention Is All You Need - Sky0x7d1" /><link rel="prev" type="text/html" href="https://careltian.github.io/posts/doc/mapreduce/" title="【论文阅读】MapReduce" /><link rel="next" type="text/html" href="https://careltian.github.io/search/" title="搜索" /><link rel="alternate" type="text/markdown" href="https://careltian.github.io/posts/doc/transformer/index.md" title="【论文阅读】Attention Is All You Need - Sky0x7d1"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "【论文阅读】Attention Is All You Need",
    "inLanguage": "zh-cn",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/careltian.github.io\/posts\/doc\/transformer\/"
    },"genre": "posts","keywords": "116, 114, 97, 110, 115, 102, 111, 114, 109, 101, 114","wordcount":  89 ,
    "url": "https:\/\/careltian.github.io\/posts\/doc\/transformer\/","datePublished": "2024-04-18T00:00:00+00:00","dateModified": "2024-04-18T00:00:00+00:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "ZergFlood"
      },"description": ""
  }
  </script><script src="/js/head/color-scheme.min.js"></script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="Sky0x7d1"><span class="header-title-text">Sky0x7d1</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a class="menu-link" href="/archives/">归档</a></li><li class="menu-item">
              <a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> 目录</a></li><li class="menu-item">
              <a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a class="menu-link" href="/me/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 关于我</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容……" id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language-switch auto d-none" aria-hidden="true">
    <span role="button" aria-label="选择语言" title="选择语言"><i class="fa-solid fa-language fa-fw" aria-hidden="true"></i></span>
    <ul class="sub-menu"><li class="menu-item active" data-type="artificial">
          <a href="/posts/doc/transformer/" data-lang="zh-cn" class="menu-link text-secondary" title="">
            <i class="fa-solid fa-person fa-fw fa-sm" aria-hidden="true"></i> </a>
        </li><li class="menu-item-divider" aria-hidden="true"></li><li class="menu-item" data-type="machine">
          <a data-lang="english" class="menu-link" title="English">
            <i class="fa-solid fa-robot fa-fw fa-sm" aria-hidden="true"></i> English</a>
        </li></ul>
  </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Sky0x7d1"><span class="header-title-text">Sky0x7d1</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容……" id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li class="menu-item"><a class="menu-link" href="/archives/">归档</a></li><li class="menu-item"><a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> 目录</a></li><li class="menu-item"><a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item"><a class="menu-link" href="/me/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 关于我</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="切换主题"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span><span id="translate" class="menu-system-item language-switch auto d-none" aria-hidden="true">
    <span role="button" aria-label="选择语言" title="选择语言" data-current=""><i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i></span>
  </span></li>
      </ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container"><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label="合集"></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>【论文阅读】Attention Is All You Need</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><img loading="lazy" src="/img/profile1.png" alt="ZergFlood" data-title="ZergFlood" width="20" height="20" class="avatar" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&nbsp;ZergFlood</span></span><span class="post-included-in">&nbsp;收录于 <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" class="post-category" title="分类 - 论文阅读"><i class="fa-regular fa-folder fa-fw" aria-hidden="true"></i> 论文阅读</a></span></div><div class="post-meta-line"><span title="发布于 2024-04-18 00:00:00"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden="true"></i><time datetime="2024-04-18">2024-04-18</time></span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#31-嵌入层">3.1 嵌入层</a></li>
        <li><a href="#32-多头注意力">3.2 多头注意力</a></li>
        <li><a href="#33-前馈神经网络">3.3 前馈神经网络</a></li>
        <li><a href="#34-注意力在模型中的应用">3.4 注意力在模型中的应用</a></li>
      </ul>
    </li>
    <li><a href="#4-训练">4 训练</a></li>
    <li><a href="#5-评价">5 评价</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><h2 id="1-论文链接" class="heading-element"><span>1 论文链接</span>
  <a href="#1-%e8%ae%ba%e6%96%87%e9%93%be%e6%8e%a5" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p><a href="https://arxiv.org/abs/1706.03762"target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/abs/1706.03762</a></p>
<h2 id="2-简介" class="heading-element"><span>2 简介</span>
  <a href="#2-%e7%ae%80%e4%bb%8b" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>早期普遍使用RNN来处理翻译任务，它是把输入和输出序列化为token，然后对每个token逐步计算。输入为上一轮计算的状态和当前的输入token，输出为下一轮的状态和输出token。所谓token就是一个单词或一个分词，起始符，终止符等组成，然后映射到一个具体的数值表中。这个结构的输入和输出也被称为编码器-解码器架构。</p>
<p>RNN的主要问题在于编码器和解码器中间通过一个状态传递信息，在处理长序列问题中计算效率和表现不太行，而且必须串行执行，效率低。于是，作者提出了基于注意力机制Transformer架构。它能够并行化训练，在8块P100训练12小时就能取得不错的效果。</p>
<h2 id="3-模型结构" class="heading-element"><span>3 模型结构</span>
  <a href="#3-%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>编码器由6个相同的层组成，图中仅展示了一个子层，每个子层中第一个是多头注意力机制捕捉不同位置之间的全局依赖关，第二个是全连接前馈神经网络主要用于非线性变换和特征提取。两个子层都使用了 层归一化和残差连接来保持梯度稳定并提升训练效率，可能是为了在深层网络中保持有效梯度传递和加速收敛。全连接层的作用是为了低维数据(512)扩展到高维（2048）输出。</p>
<figure style="text-align: center;"><a class="lightgallery" href="/img/Transformer_Architecture.jpg?size=large" data-thumbnail="/img/Transformer_Architecture.jpg?size=small" data-sub-html="<h2>图一</h2>"><img loading="lazy" src="/img/Transformer_Architecture.jpg" alt="图一" srcset="/img/Transformer_Architecture.jpg?size=small, /img/Transformer_Architecture.jpg?size=medium 1.5x, /img/Transformer_Architecture.jpg?size=large 2x" data-title="图一" width="300" style="--width: 912px;--aspect-ratio: 912 / 1320;background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></a></figure>
<h3 id="31-嵌入层" class="heading-element"><span>3.1 嵌入层</span>
  <a href="#31-%e5%b5%8c%e5%85%a5%e5%b1%82" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>图中的embedding layer来源过程如下，句子-&gt;token化-&gt;token ID-&gt;嵌入层（d=512)，这样一个词就由一个512维的稠密向量，例如[1.212, 3.234, 5.111, &hellip;]所对应的表示“我”。这样一段句子本质上就是一个嵌入矩阵作为输入。Positional Encoding是为了捕捉单词之间的顺序关系，计算公式如下，pos是位置，i是嵌入向量的索引，d=512。
\begin{align}
PE_{(pos, 2i)} &amp;= \sin \left( \frac{pos}{10000^{\frac{2i}{d}}} \right) \
PE_{(pos, 2i+1)} &amp;= \cos \left( \frac{pos}{10000^{\frac{2i}{d}}} \right)
\end{align}</p>
<h3 id="32-多头注意力" class="heading-element"><span>3.2 多头注意力</span>
  <a href="#32-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>文中的多头注意力是由8个缩放点乘积注意力合并得到的，先说缩放点积注意力。Q,K,V是一组抽象的概念，Q直观说就是生成的方向，K用于输入信息匹配，QK内积得到当前位置哪个输出信息最相关，一轮计算得到两个内容，注意力输出表示全局上下文重要信息，权重表示查询位置Q对K的重要程度。
\begin{align}
\text{Attention}(Q, K, V) &amp;= \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\end{align}</p>
<h3 id="33-前馈神经网络" class="heading-element"><span>3.3 前馈神经网络</span>
  <a href="#33-%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>扩大维度至2048，捕捉更复杂的特征。Relu将函数变为非线形运算是精髓，是产生智能学习质变的关键之一。</p>
<p>\begin{align}
FFN(x) &amp;= \max(0, x W_1 + b_1) W_2 + b_2
\end{align}</p>
<h3 id="34-注意力在模型中的应用" class="heading-element"><span>3.4 注意力在模型中的应用</span>
  <a href="#34-%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%9c%a8%e6%a8%a1%e5%9e%8b%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>文中用自注意力层和循环层，卷积层做比较，都是把一个向量映射到另一个向量，比如隐藏层在编码器和解码器之间的作用。通过对计算复杂度，并行化，最长路径网络的比较，自注意力的时间复杂度明显是比其余两者更低的。</p>
<p>Tranformer运用多头注意力在下面三个方面</p>
<ul>
<li>在编码器-解码器层中，Q来自于上一个解码层的输出，而K,V来源于上一个解码层输出。这样使得解码器的每个位置都能获取输入序列的所有位置。</li>
<li>编码器中的自注意力层，所有的Q，K，V来自于相同的来源。编码器中，它们来源于前一层的输出，第一层来源于嵌入层。这样做生成某个位置的单词时可以考虑输入序列的任意位置单词，捕捉长距离依赖关系。</li>
<li>解码器的自注意力层同样能能关注到第一个位置到当前输出位置的信息，为了保证自回归，训练中不能提前知道未来的词，将未来位置的注意力权重设置负无穷，softmax就会输出0的概率。</li>
</ul>
<h2 id="4-训练" class="heading-element"><span>4 训练</span>
  <a href="#4-%e8%ae%ad%e7%bb%83" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>使用了Adam优化器，β1 = 0.9, β2 = 0.98 and ϵ= 10−9和动态学习率算法。</p>
<p>正则化方法，残差连接+每个子层dropout概率0.1+标签平滑处理</p>
<h2 id="5-评价" class="heading-element"><span>5 评价</span>
  <a href="#5-%e8%af%84%e4%bb%b7" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>相比于RNN和CNN最显著的优点就是能够并行化，缩短计算时间，并且能够合适地处理长序列，捕捉上下序列重要信息。我认为他的缺点是必须要提供大量的数据集才能有较理想的效果，个人设备训练时很容易因数据集不足或模型参数少造成过拟合。</p>
</div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title="更新于 2024-04-18 00:00:00">更新于 2024-04-18&nbsp;</span>
      </div><div class="post-info-license">
            <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a></span>
          </div></div><div class="post-info-line">
        <div class="post-info-md"></div>
        <div class="post-info-share">
          <span></span>
        </div>
      </div></div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw me-1" aria-hidden="true"></i><a href="/tags/transformer/" class="post-tag" title="标签 - Transformer">Transformer</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div><div class="post-nav"><a href="/posts/doc/mapreduce/" class="post-nav-item" rel="prev" title="【论文阅读】MapReduce"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>【论文阅读】MapReduce</a><a href="/posts/doc/prediction/" class="post-nav-item" rel="next" title="互联网未来10年的展望">互联网未来10年的展望<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article>

  <aside class="toc" id="toc-auto" aria-label="目录"><h2 class="toc-title">目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside></main></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><a href="https://github.com/CarelTian" title="Visit my GitHub profile"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true" width="56" height="56"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><noscript>
    <div class="noscript-warning">该网站在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="preload" href="/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/katex/katex.min.css"></noscript><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/js/flyfish.js" defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{"enable":false},"cse":{"gotoResultsPage":"前往搜索结果页面……","resultsPage":"/search/"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":30,"type":"cse"},"version":"v0.3.17-8b402129"};</script><script src="/js/theme.min.js" defer></script><script src="/lib/translate.min.js" defer></script><script>window.ATConfig={"hugoLangCodes":["zh-cn"],"hugoLangMap":{"zh-cn":"/posts/doc/transformer/"}};</script><script src="/js/translate.fixit.min.js" defer></script></body>
</html>
